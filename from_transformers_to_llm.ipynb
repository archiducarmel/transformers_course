{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkgJFr82sCYyUBgWV0/DYo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/archiducarmel/transformers_course/blob/main/from_transformers_to_llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans les chapitres précédents, nous avons découvert l'architecture fascinante des transfomers, la place de l'auto-attention ainsi que les déclinaisons qui pouvaient être réalisés à partir d'un modèle transformer.\n",
        "\n",
        "Mais les avantages induits par cette architecture révolutionnaire sont multiples et ont permis un saut technologique supplémentaires dans unpassé récent."
      ],
      "metadata": {
        "id": "io8nlts0DzuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rappel des caractéristiques des architectures transformers"
      ],
      "metadata": {
        "id": "LRD58sicFKS2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mécanisme d'Attention Multi-Tête\n",
        "\n",
        "Les Transformers utilisent une mécanique d'attention multi-tête, qui permet au modèle de considérer simultanément différentes parties de la séquence d'entrée.\n",
        "Chaque tête d'attention calcule une pondération sur les différents éléments de la séquence en fonction de leur importance relative, fournissant ainsi une représentation plus riche et contextualisée.\n",
        "\n",
        "![](https://production-media.paperswithcode.com/methods/multi-head-attention_l1A3G7a.png)"
      ],
      "metadata": {
        "id": "N21ov73yDOS1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parallélisme\n",
        "\n",
        "L'architecture des Transformers permet un calcul massivement parallèle. Les opérations d'attention pour chaque tête peuvent être effectuées indépendamment, facilitant l'exploitation des capacités de calcul parallèle des GPU.\n",
        "Cela accélère considérablement l'entraînement et l'inférence, ce qui en fait une solution efficace pour traiter de grandes quantités de données textuelles.\n",
        "\n",
        "![](https://d1fmx1rbmqrxrr.cloudfront.net/zdnet/optim/i/edit/ne/2021/03/GPU__w630.jpg)\n"
      ],
      "metadata": {
        "id": "93BpbJNyDUjy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Capacité de Traitement de Séquences Variables\n",
        "\n",
        "Les Transformers sont capables de gérer des séquences de longueurs variables, une caractéristique cruciale pour le traitement du langage naturel où les phrases peuvent avoir des longueurs différentes.\n",
        "L'attention auto-pondérée permet au modèle de s'ajuster dynamiquement à la longueur de la séquence, sans nécessiter de modifications architecturales.\n"
      ],
      "metadata": {
        "id": "5ySOFyzpDYcp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modélisation Contextuelle\n",
        "\n",
        "Les Transformers effectuent une modélisation contextuelle des mots, capturant ainsi le contexte global de la séquence.\n",
        "Chaque mot est représenté en fonction du contexte de l'ensemble de la séquence, améliorant la compréhension et la génération de texte par rapport aux modèles antérieurs.\n"
      ],
      "metadata": {
        "id": "BlLzqFrDDaUF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Auto-attention et Résolution des Problèmes de Dépendance à Long Terme\n",
        "\n",
        "L'auto-attention permet aux Transformers de traiter efficacement les dépendances à long terme dans les séquences, ce qui est crucial pour comprendre et générer du texte de manière cohérente.\n",
        "La capacité à se focaliser sur des parties spécifiques de la séquence sans dégradation des performances avec des séquences plus longues a été une avancée majeure.\n"
      ],
      "metadata": {
        "id": "a4qTyIqBDmt-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comment utiliser efficacement les transformers ?"
      ],
      "metadata": {
        "id": "otkN7IXzKBaO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Profiter du transfer learning et des capacités de généralisation :\n",
        "\n",
        "Avec les transformers, le domaine du NLP et du deep learning ont franchi une nouvelle étape en termes de profondeur de couches et de nombre de paramètres. Le transformer le plus simple possède environ 10 millions de paramètres. En comparaison, un modèle LSTM similaire contient 50 fois moins de paramètres.\n",
        "\n",
        "Avec un si grand nombre de paramètres, il est plus pertinent de pré-entrainer les modèles transformers plutôt que de ré-entrainer les modèles pour des tâches spécifiques\n",
        "\n",
        "Les transformers associés au transfer learning ont démontré leur capacité à bien fonctionner sur des quantités massives de données textuelles, permettant ainsi l'entraînement de modèles très grands sur diverses tâches.\n",
        "Les modèles pré-entraînés sur de vastes corpus de données peuvent être fine-tunés pour des tâches spécifiques, montrant une capacité de généralisation impressionnante.\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:960/1*OawILeEuxDJj--6wENKHRg.jpeg)\n"
      ],
      "metadata": {
        "id": "jGXatEF3Dezr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bénéficier de la flexibilité et de l'adaptabilité de l'architecture\n",
        "\n",
        "Les Transformers sont flexibles et peuvent être adaptés à diverses tâches en ajustant simplement leur architecture. Des modèles spécifiques (encodeurs seuls, décodeurs seuls, encodeurs + décodeurs) peuvent être déclinés.\n",
        "Cette adaptabilité permet d'utiliser une architecture de base pour résoudre une multitude de problèmes liés au langage.\n",
        "\n"
      ],
      "metadata": {
        "id": "6A7hGkuwDtq-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# L'avènement des LLM : des architectures adaptables\n",
        "\n",
        "En bénéficiant de l'adaptabilité de l'architecture et du pré-entrainement, l'architecture des transformers a été fondamentale pour le développement des LLM tels que GPT, BERT, T5, etc., démontrant la polyvalence et l'efficacité de cette approche dans la modélisation des relations complexes dans le langage naturel."
      ],
      "metadata": {
        "id": "hrXvW-c6DyDL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer: Encoder-Decoder\n",
        "\n",
        "L'architecture encodeur-décodeur du transformer est utilisée pour des tâches telles que la traduction automatique, où le modèle doit traiter en entrée une phrase dans une langue et générer une phrase dans une autre langue. L'encodeur prend la phrase d'entrée et en produit une représentation vectorielle de taille fixe, qui est ensuite introduite dans le décodeur pour générer la phrase de sortie. Le décodeur utilise à la fois l'auto-attention et l'attention croisée, où le mécanisme d'attention est appliqué à la sortie de l'encodeur et à l'entrée du décodeur.\n",
        "\n",
        "![](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/encoder_decoder/EncoderDecoder.png)\n",
        "\n",
        "L'un des modèles d'encodeur-décodeur de transformer les plus populaires est le T5 (Text-to-Text Transfer Transformer), qui a été introduit par Google en 2019. Le T5 peut être réglé avec précision pour un large éventail de tâches NLP, notamment la traduction automatique, réponses aux questions, génération de résumé, etc.\n",
        "\n",
        "Des exemples concrets de l'architecture d'encodeur-décodeur de transformateur incluent Google Translate, qui utilise le modèle T5 pour traduire du texte entre les langues, et le M2M-100 de Facebook, un modèle de traduction automatique multilingue massif capable de traduire entre 100 langues différentes."
      ],
      "metadata": {
        "id": "Bf1PsxFKw6fV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer: Encoder\n",
        "\n",
        "L'architecture de l'encodeur du transformer est utilisée pour des tâches telles que la classification de texte, où le modèle doit classer un bout de texte dans l'une des nombreuses catégories prédéfinies, telles que l'analyse des sentiments, la classification de sujets ou la détection de spam. L'encodeur prend une séquence de tokens et produit une représentation vectorielle de taille fixe de la séquence entière, qui peut ensuite être utilisée pour la classification.\n",
        "\n",
        "L'un des modèles d'encodeurs de transformateur les plus populaires est BERT (Bidirectionnel Encoder Representations from Transformers), introduit par Google en 2018. BERT est pré-entraîné sur de grandes quantités de données textuelles et peut être affiné pour un large éventail de tâches NLP.\n",
        "\n",
        "![](https://iq.opengenus.org/content/images/2020/06/Screenshot-from-2020-06-14-16-13-21.png)\n",
        "\n",
        "Contrairement à l'architecture encodeur-décodeur, l'encodeur du transformer ne s'intéresse qu'à la séquence d'entrée et ne génère aucune séquence de sortie. Il applique un mécanisme d'auto-attention aux tokens d'entrée, lui permettant de se concentrer sur les parties les plus pertinentes de l'entrée pour la tâche donnée.\n",
        "\n",
        "Des exemples concrets de l'architecture de l'encodeur du transformer incluent l'analyse des sentiments, où le modèle doit classer un avis donné comme positif ou négatif, et la détection du spam par courrier électronique, où le modèle doit classer un e-mail donné comme spam ou non.\n"
      ],
      "metadata": {
        "id": "sJ6hq-PBzAtx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer: Decoder\n",
        "\n",
        "L'architecture du décodeur du transformer est utilisée pour des tâches telles que la génération de texte, où le modèle doit générer une séquence de mots basée sur une invite de saisie (prompt) ou un contexte. Le décodeur prend en compte une représentation vectorielle de taille fixe du contexte et l'utilise pour générer une séquence de mots un par un, chaque mot étant conditionné par les mots générés précédemment.\n",
        "\n",
        "L'un des modèles de décodeur de transformer les plus populaires est le GPT-3 (Generative Pre-trained Transformer 3), qui a été introduit par OpenAI en 2020. Le GPT-3 est un modèle de langage massif qui peut générer du texte de type humain dans une large gamme. de styles et de genres.\n",
        "\n",
        "![](https://pbs.twimg.com/ext_tw_video_thumb/1286004942508109824/pu/img/awrq9Auq-GhDYFH9.jpg:large)\n",
        "\n",
        "L'architecture du décodeur de transformer introduit une technique appelée masquage triangulaire pour l'attention, qui garantit que le mécanisme d'attention ne regarde que les token à gauche du jeton actuellement généré. Cela empêche le modèle de « tricher » en regardant des tokens qu'il n'a pas encore générés.\n",
        "\n",
        "Des exemples concrets de l'architecture du décodeur de transformer incluent la génération de texte, où le modèle doit générer une histoire ou un article basé sur une invite ou un sujet donné, et les chatbots, où le modèle doit générer des réponses aux entrées de l'utilisateur de manière naturelle et engageante."
      ],
      "metadata": {
        "id": "y73gyeba3g_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# L'avènement des LLM : l'explosion du nombre de paramètres\n",
        "\n",
        "Les modèles LLM dépendent principalement du volume de données et du nombre de paramètres sur lesquels ils sont entrainés. En informatique et dans le domaine du cloud computing, la loi de Moore est une hypothèse selon laquelle les performances des modèles de traitement de données continueront à s'améliorer de façon exponentielle, suivant une tendance similaire à la loi de Moore concernant le nombre de transistors sur une carte de calcul informatique.\n",
        "\n",
        "Vous pouvez observer l'augmentation exponentielle de la taille du modèle à partir du graphique ci-dessous. Selon la loi de Moore, la taille du modèle augmente d’un facteur 10 d’une année sur l’autre.\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:1128/0*rnbAsKj7LGnMWKH4)\n"
      ],
      "metadata": {
        "id": "FYGKntfr45iZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# L'avènement des LLM : les composants de base d'un LLM\n",
        "\n",
        "Pour traiter et comprendre efficacement les données en langage naturel, les grands modèles linguistiques (LLM) intègrent divers composants fondamentaux.\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:828/format:webp/1*mekXmmtvgV1Not3AMiJO7w.png)"
      ],
      "metadata": {
        "id": "yfI7gW8Y7fvo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization\n",
        "\n",
        "La tokenisation dans les grands modèles linguistiques (LLM) implique de diviser le texte en unités plus petites appelées **tokens**. Son objectif est de créer une représentation standardisée pour un traitement et une analyse efficaces. Les LLM utilisent la tokenisation dans des tâches telles que la traduction automatique, l'analyse des sentiments et la réponse aux questions pour extraire des variables et optimiser les ressources informatiques."
      ],
      "metadata": {
        "id": "hMDTnEFc734c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding\n",
        "\n",
        "Les embeddings dans de grands modèles linguistiques (LLM) visent à représenter des mots ou des tokens sous forme de vecteurs denses dans un espace continu.\n",
        "\n",
        "L'objectif est de capturer les relations sémantiques et les informations contextuelles. Les LLM utilisent les embeddings pour des applications telles que la similarité de mots, l'analyse des sentiments et la classification de textes afin d'améliorer la compréhension du langage et l'extraction d'informations."
      ],
      "metadata": {
        "id": "X5bPOU0B8qzV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention\n",
        "\n",
        "L'attention dans les grands modèles linguistiques (LLM) fait référence au mécanisme qui permet au modèle de se concentrer sur les parties pertinentes de la séquence d'entrée. L'objectif est d'attribuer des pondérations d'importance à différents éléments, en capturant efficacement les dépendances contextuelles. L'attention est appliquée à des tâches telles que la traduction automatique, la synthèse de texte et l'analyse des sentiments, améliorant ainsi la capacité du modèle à traiter les dépendances à long terme et à améliorer les performances."
      ],
      "metadata": {
        "id": "pfM-AbUt9ZoN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pré-entrainement\n",
        "\n",
        "Le pré-entrainement des grands modèles linguistiques (LLM) implique l'entrainement d'un modèle sur un vaste corpus de texte non annoté pour apprendre les représentations linguistiques générales.\n",
        "\n",
        "L’objectif est de capturer les propriétés statistiques du langage et de les encoder dans les paramètres du modèle. Le pré-entrainement sert de base aux tâches en aval telles que la génération de texte, l'analyse des sentiments et la réponse aux questions, permettant un meilleur apprentissage par transfert et de meilleures performances."
      ],
      "metadata": {
        "id": "WmN4aSl1941W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transfer learning\n",
        "\n",
        "L'apprentissage par transfert dans les grands modèles linguistiques (LLM) fait référence au processus consistant à exploiter les connaissances acquises lors du pré-entrainement sur une tâche et à les appliquer pour améliorer les performances sur une autre tâche en aval.\n",
        "\n",
        "L’objectif est de transférer des représentations apprises et des connaissances linguistiques. L'apprentissage par transfert est appliqué à diverses tâches de traitement du langage naturel, notamment l'analyse des sentiments, la reconnaissance d'entités nommées et la traduction linguistique, pour améliorer les performances du modèle, réduire le temps d'entrainement et atténuer le besoin de grands ensembles de données étiquetés."
      ],
      "metadata": {
        "id": "q4hFTa6O-dpE"
      }
    }
  ]
}